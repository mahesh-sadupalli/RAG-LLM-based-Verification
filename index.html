<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>RAG-assisted LLM-based Verification</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>

  <h1> RAG-assisted LLM-based Verification</h1>
  <p><strong>By Mahesh Sadupalli, M.Sc. Artificial Intelligence</strong></p>
  <p>
    <strong>Mentor:</strong> Dr. Mahdi Taheri<br>
    <strong>First Examiner:</strong> Dr.-Ing. habil. Christian Herglotz (Vertretungsprofessur)<br>
    <strong>Chair:</strong> Fakultät 1 – Fachgebiet Technische Informatik, BTU Cottbus-Senftenberg
  </p>

  <div class="section">
  <h2> Introduction</h2>
  <p>
    Large Language Models (LLMs) like GPT-4 and Claude have transformed how we generate and understand human-like language.
    However, they come with certain limitations:
  </p>
  <ul>
    <li> May hallucinate or generate inaccurate outputs</li>
    <li> Cannot access real-time or domain-specific external data</li>
    <li> Performance degrades in high-stakes or technical domains without training</li>
  </ul>
  <p>
    <strong>Retrieval-Augmented Generation (RAG)</strong> addresses these limitations by combining LLMs with external search:
  </p>
  <ul>
    <li> A retriever selects relevant knowledge from a document store or database</li>
    <li> The LLM generates responses using the retrieved context</li>
  </ul>
  <p>
    This project applies the RAG+LLM architecture to <strong>hardware verification</strong> and <strong>EDA workflows</strong>, enabling smarter, explainable, and more accurate analysis pipelines.
  </p>
</div>

  <div class="section">
    <h2>Project Overview</h2>
    <p>Large Language Models (LLMs) like GPT or LLaMA are impressive in language generation, but their general training data often lacks the precision and depth required for technical domains like Electronic Design Automation (EDA). This mismatch results in hallucinations and limited reasoning when applied to tasks like formal verification, waveform interpretation, or RTL debugging.</p>

    <p>Our project addresses this limitation by integrating <strong>Retrieval-Augmented Generation (RAG)</strong> into the LLM pipeline. By connecting LLMs to a curated knowledge base of EDA documents—such as SystemVerilog assertions, waveform logs, and simulation outputs—we empower them to reason with up-to-date, accurate, and domain-specific information.</p>

    <p>This hybrid system not only improves factual accuracy but also introduces transparency through Explainable AI (XAI) techniques, offering insights into how the model arrived at each conclusion. Whether it’s detecting design bugs or suggesting testbench improvements, this tool represents a leap forward in intelligent automation for chip design.</p>

    <p><em>Explore the source code, architecture, and example queries on our GitHub repository.</em></p>

    <div class="highlight">
      Applications include:
      <ul>
        <li>Formal Verification</li>
        <li>Mutation Testing</li>
        <li>Bug Detection and Fixing</li>
        <li>Explainability in Verification</li>
        <li>RISC-V Integration</li>
      </ul>
    </div>
  </div>

  <div class="section">
    <h2>Architecture Snapshot</h2>
    <p><em>(Coming soon!)</em> We'll upload the end-to-end RAG+LLM pipeline diagram that illustrates:</p>
    <ul>
      <li>LangChain or RAG backend</li>
      <li>Interaction with Z3 SMT solver or Verilog test cases</li>
      <li>Human-in-the-loop debugging</li>
    </ul>
  </div>

  <div class="section">
    <h2>Related Technologies</h2>
    <ul>
      <li>LLMs (GPT-4, Claude, LLaMA)</li>
      <li>LangChain</li>
      <li>SystemVerilog + Assertions (SVA)</li>
      <li>Z3 Theorem Prover</li>
      <li>Word2Vec + LDA (for semantic drift detection)</li>
    </ul>
  </div>
  <div class="section">
  <h2> Research Papers</h2>

  <h3>1. Dataset</h3>
  <ul>
    <li><a href="#">VERT: A SystemVerilog Assertion Dataset to Improve Hardware Verification with LLMs</a></li>
    <li><a href="#">AssertionBench: A Benchmark to Evaluate LLMs for Assertion Generation</a></li>
    <li><a href="#">OpenLLM-RTL: Open Dataset and Benchmark for RTL Generation</a></li>
    <li><a href="#">VerilogDB: High-Quality Dataset for RTL Generation with Preprocessing</a></li>
  </ul>

  <h3>2. LLM Verification</h3>
  <ul>
    <li><a href="#">LLM Verification</a></li>
    <li><a href="#">LLMs for Hardware Verification</a></li>
    <li><a href="#">LLM-Guided Formal Verification</a></li>
    <li><a href="#">AssertLLM: Hardware Verification Assertions via Multi-LLMs</a></li>
    <li><a href="#">Continual Learning of Large Language Models – A Comprehensive Survey</a></li>
    <li><a href="#">Towards Incremental Learning in LLMs – A Critical Review</a></li>
  </ul>

  <h3>3. Explainable AI (XAI)</h3>
  <ul>
    <li><a href="#">Concept Drift: Survey on Monitoring Evolving Environments</a></li>
    <li><a href="#">Benchmark of Unsupervised Drift Detectors on Real-World Data Streams</a></li>
  </ul>

  <h3>4. Foundational & Other Papers</h3>
  <ul>
    <li><a href="#">Word2Vec: Efficient Estimation of Word Representations</a> – Mikolov et al., Google</li>
    <li><a href="#">Attention Is All You Need</a> – Vaswani et al., 2017</li>
    <li><a href="#">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (RAG)</a> – Lewis et al., Facebook AI</li>
    <li><a href="#">LLaMA: Open and Efficient Foundation Language Models</a> – Meta AI</li>
    <li><a href="#">HybridRAG: Integrating Knowledge Graphs with Vector Retrieval</a></li>
    <li><a href="#">LDA + Word2Vec: Topic Evolution of Mental Models</a></li>
  </ul>
</div>


  <div class="section">
    <h2>Contact</h2>
    <p>Questions or collaborations? Reach out via GitHub or email: <strong>maheshsadupalli [at] gmail.com</strong></p>
  </div>

</body>
</html>
<--help Trigger rebuild -->
