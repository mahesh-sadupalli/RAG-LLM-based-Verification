<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>RAG-assisted LLM-based Verification</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>

  <h1> RAG-assisted LLM-based Verification</h1>
  <p><strong>By Mahesh Sadupalli, M.Sc. Artificial Intelligence</strong></p>
  <p>
    <strong>Mentor:</strong> Dr. Mahdi Taheri<br>
    <strong>First Examiner:</strong> Dr.-Ing. habil. Christian Herglotz (Vertretungsprofessur)<br>
    <strong>Chair:</strong> FakultÃ¤t 1 â€“ Fachgebiet Technische Informatik, BTU Cottbus-Senftenberg
  </p>

  <div class="section">
  <h2> Introduction</h2>
  <p>
    Large Language Models (LLMs) like GPT-4 and Claude have transformed how we generate and understand human-like language.
    However, they come with certain limitations:
  </p>
  <ul>
    <li> May hallucinate or generate inaccurate outputs</li>
    <li> Cannot access real-time or domain-specific external data</li>
    <li> Performance degrades in high-stakes or technical domains without training</li>
  </ul>
  <p>
    <strong>Retrieval-Augmented Generation (RAG)</strong> addresses these limitations by combining LLMs with external search:
  </p>
  <ul>
    <li> A retriever selects relevant knowledge from a document store or database</li>
    <li> The LLM generates responses using the retrieved context</li>
  </ul>
  <p>
    This project applies the RAG+LLM architecture to <strong>hardware verification</strong> and <strong>EDA workflows</strong>, enabling smarter, explainable, and more accurate analysis pipelines.
  </p>
</div>

  <div class="section">
    <h2>Project Overview</h2>
    <p>Large Language Models (LLMs) like GPT or LLaMA are impressive in language generation, but their general training data often lacks the precision and depth required for technical domains like Electronic Design Automation (EDA). This mismatch results in hallucinations and limited reasoning when applied to tasks like formal verification, waveform interpretation, or RTL debugging.</p>

    <p>Our project addresses this limitation by integrating <strong>Retrieval-Augmented Generation (RAG)</strong> into the LLM pipeline. By connecting LLMs to a curated knowledge base of EDA documentsâ€”such as SystemVerilog assertions, waveform logs, and simulation outputsâ€”we empower them to reason with up-to-date, accurate, and domain-specific information.</p>

    <p>This hybrid system not only improves factual accuracy but also introduces transparency through Explainable AI (XAI) techniques, offering insights into how the model arrived at each conclusion. Whether itâ€™s detecting design bugs or suggesting testbench improvements, this tool represents a leap forward in intelligent automation for chip design.</p>

    <p><em>Explore the source code, architecture, and example queries on our GitHub repository.</em></p>

    <div class="highlight">
      Applications include:
      <ul>
        <li>Formal Verification</li>
        <li>Mutation Testing</li>
        <li>Bug Detection and Fixing</li>
        <li>Explainability in Verification</li>
        <li>RISC-V Integration</li>
      </ul>
    </div>
  </div>

  <div class="section">
    <h2>Architecture Snapshot</h2>
    <p><em>(Coming soon!)</em> We'll upload the end-to-end RAG+LLM pipeline diagram that illustrates:</p>
    <ul>
      <li>LangChain or RAG backend</li>
      <li>Interaction with Z3 SMT solver or Verilog test cases</li>
      <li>Human-in-the-loop debugging</li>
    </ul>
  </div>

  <div class="section">
    <h2>Related Technologies</h2>
    <ul>
      <li>LLMs (GPT-4, Claude, LLaMA)</li>
      <li>LangChain</li>
      <li>SystemVerilog + Assertions (SVA)</li>
      <li>Z3 Theorem Prover</li>
      <li>Word2Vec + LDA (for semantic drift detection)</li>
    </ul>
  </div>
  <div class="section">
  <h2>ðŸ“š Research Papers</h2>

  <h3>1. Dataset</h3>
  <ul>
    <li><a href="papers/VERT-ASYSTEMVERILOGASSERTIONDATASETTO IMPROVEHARDWAREVERIFICATIONWITHLLMS.pdf" target="_blank">VERT: A SystemVerilog Assertion Dataset to Improve Hardware Verification with LLMs</a></li>
    <li><a href="papers/AssertionBench- A Benchmark to Evaluate Large-Language Models for Assertion Generation for Hardware Design.pdf" target="_blank">AssertionBench: A Benchmark for LLMs in Assertion Generation</a></li>
    <li><a href="papers/OpenLLM-RTL- Open Dataset and Benchmark for LLM-Aided Design RTL Generation.pdf" target="_blank">OpenLLM-RTL: Open Dataset & Benchmark for RTL Generation</a></li>
    <li><a href="papers/VerilogDB- The Largest, Highest-Quality Dataset with a Preprocessing Framework for LLM-based RTL Generation.pdf" target="_blank">VerilogDB: Preprocessed Dataset for LLM RTL Generation</a></li>
  </ul>

  <h3>2. LLM Verification</h3>
  <ul>
    <li><a href="papers/1 LLM Verification.pdf" target="_blank">LLM Verification</a></li>
    <li><a href="papers/Â§ LLMs for Hardware verification .pdf" target="_blank">LLMs for Hardware Verification</a></li>
    <li><a href="papers/6 LLM guided formal verification .pdf" target="_blank">LLM-Guided Formal Verification</a></li>
    <li><a href="papers/AssertLLM- Generating Hardware Verification Assertions from Design Specifications via Multi-LLMs.pdf" target="_blank">AssertLLM: Assertion Generation with Multi-LLMs</a></li>
    <li><a href="papers/Continual Learning of Large Language Models- A Comprehensive Survey.pdf" target="_blank">Continual Learning of LLMs â€“ Survey</a></li>
    <li><a href="papers/Towards Incremental Learning in Large   Language Models- A Critical Review.pdf" target="_blank">Incremental Learning in LLMs â€“ Critical Review</a></li>
  </ul>

  <h3>3. Explainable AI (XAI)</h3>
  <ul>
    <li><a href="papers/One or Two Things We know about Concept Drift  A Survey on Monitoring Evolving Environments.pdf" target="_blank">Survey on Concept Drift Monitoring</a></li>
    <li><a href="papers/A benchmark and survey of fully unsupervised concept drift detectors on real-world data streams.pdf" target="_blank">Unsupervised Drift Detection â€“ Benchmark and Survey</a></li>
  </ul>

  <h3>4. Foundational & Other Papers</h3>
  <ul>
    <li><a href="papers/Word2Vec.pdf" target="_blank">Word2Vec â€“ Google Research</a></li>
    <li><a href="papers/Attention.pdf" target="_blank">Attention Is All You Need</a></li>
    <li><a href="papers/RAG.pdf" target="_blank">Retrieval-Augmented Generation (RAG) â€“ Facebook AI</a></li>
    <li><a href="papers/LLaMA.pdf" target="_blank">LLaMA: Open and Efficient Foundation Models â€“ Meta AI</a></li>
    <li><a href="papers/HybridRAG- Integrating Knowledge Graphs and Vector Retrieval   Augmented Generation for Efficient Information Extraction.pdf" target="_blank">HybridRAG: Knowledge Graph + Vector Retrieval</a></li>
    <li><a href="papers/An integrated latent Dirichlet allocation and Word2vec method for generating the topic evolution of mental models from global to local.pdf" target="_blank">Topic Evolution with LDA + Word2Vec</a></li>
  </ul>
</div>



  <div class="section">
    <h2>Contact</h2>
    <p>Questions or collaborations? Reach out via GitHub or email: <strong>maheshsadupalli [at] gmail.com</strong></p>
  </div>

</body>
</html>
<--help Trigger rebuild -->
